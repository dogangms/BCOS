"""
Decentralized AI Node Operating System - Step 4: File System Implementation
A comprehensive virtual file system with AI/blockchain optimizations and security features.
"""

import os
import time
import json
import uuid
import hashlib
import threading
from enum import Enum
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import deque
import random
import pickle
from pathlib import Path

class FileType(Enum):
    """File types for AI/blockchain specific categorization"""
    REGULAR = "ðŸ“„ Regular File"
    DIRECTORY = "ðŸ“ Directory"
    AI_MODEL = "ðŸ§  AI Model"
    AI_DATASET = "ðŸ“Š AI Dataset"
    BLOCKCHAIN_DATA = "â›“ï¸ Blockchain Data"
    SMART_CONTRACT = "ðŸ“œ Smart Contract"
    NETWORK_CONFIG = "ðŸŒ Network Config"
    SYSTEM_FILE = "âš™ï¸ System File"
    ENCRYPTED = "ðŸ”’ Encrypted File"
    LINK = "ðŸ”— Symbolic Link"

class FilePermission(Enum):
    """File permission levels"""
    READ = 0x4
    WRITE = 0x2
    EXECUTE = 0x1

class AccessLevel(Enum):
    """User access levels for AI/blockchain operations"""
    ADMIN = "ðŸ”´ Administrator"
    AI_ENGINEER = "ðŸ§  AI Engineer"
    BLOCKCHAIN_DEV = "â›“ï¸ Blockchain Developer"
    USER = "ðŸ‘¤ User"
    GUEST = "ðŸ‘¥ Guest"

@dataclass
class Block:
    """File system block (4KB default)"""
    block_id: int
    size: int = 4096
    data: bytes = field(default_factory=bytes)
    used: bool = False
    dirty: bool = False
    references: int = 0
    last_accessed: float = field(default_factory=time.time)
    checksum: Optional[str] = None
    
    def calculate_checksum(self) -> str:
        """Calculate SHA-256 checksum of block data"""
        return hashlib.sha256(self.data).hexdigest()
    
    def verify_integrity(self) -> bool:
        """Verify block data integrity"""
        return self.checksum == self.calculate_checksum()

@dataclass
class FileMetadata:
    """File metadata and attributes"""
    name: str
    file_type: FileType
    size: int = 0
    created_time: float = field(default_factory=time.time)
    modified_time: float = field(default_factory=time.time)
    accessed_time: float = field(default_factory=time.time)
    owner: str = "system"
    access_level: AccessLevel = AccessLevel.USER
    permissions: int = 0o644  # Unix-style permissions
    block_list: List[int] = field(default_factory=list)
    
    # AI/Blockchain specific metadata
    model_version: Optional[str] = None
    dataset_format: Optional[str] = None
    encryption_key: Optional[str] = None
    smart_contract_version: Optional[str] = None
    ai_accuracy: Optional[float] = None
    blockchain_hash: Optional[str] = None
    
    # Caching and performance
    cache_priority: int = 0
    pin_in_memory: bool = False
    compression_ratio: float = 1.0
    
    def has_permission(self, user_level: AccessLevel, operation: FilePermission) -> bool:
        """Check if user has permission for operation"""
        # Admin has all permissions
        if user_level == AccessLevel.ADMIN:
            return True
            
        # For testing purposes, be more lenient with permissions
        # In a real system, this would be more restrictive
        if operation == FilePermission.READ:
            return True  # Allow read access to all users
        elif operation == FilePermission.WRITE:
            # Allow write access for USER level and above
            level_hierarchy = {
                AccessLevel.ADMIN: 4,
                AccessLevel.AI_ENGINEER: 3,
                AccessLevel.BLOCKCHAIN_DEV: 3,
                AccessLevel.USER: 2,
                AccessLevel.GUEST: 1
            }
            
            required_level = level_hierarchy.get(self.access_level, 1)
            user_level_value = level_hierarchy.get(user_level, 1)
            
            return user_level_value >= 2  # USER level or higher can write
        elif operation == FilePermission.EXECUTE:
            return True  # Allow execute for all users
            
        return True  # Default to allow for testing

@dataclass
class DirectoryEntry:
    """Directory entry structure"""
    name: str
    is_directory: bool
    file_id: str
    size: int = 0
    modified_time: float = field(default_factory=time.time)

class FileSystemCache:
    """LRU-based file system cache for performance optimization"""
    
    def __init__(self, max_size: int = 50):
        self.max_size = max_size
        self.cache: Dict[str, bytes] = {}
        self.access_order: deque = deque()
        self.lock = threading.RLock()
        
    def get(self, file_id: str) -> Optional[bytes]:
        """Get file data from cache"""
        with self.lock:
            if file_id in self.cache:
                # Move to end (most recently used)
                self.access_order.remove(file_id)
                self.access_order.append(file_id)
                return self.cache[file_id]
            return None
            
    def put(self, file_id: str, data: bytes):
        """Put file data in cache"""
        with self.lock:
            if file_id in self.cache:
                # Update existing entry
                self.access_order.remove(file_id)
            elif len(self.cache) >= self.max_size:
                # Remove least recently used
                oldest = self.access_order.popleft()
                del self.cache[oldest]
                
            self.cache[file_id] = data
            self.access_order.append(file_id)
            
    def remove(self, file_id: str):
        """Remove file from cache"""
        with self.lock:
            if file_id in self.cache:
                del self.cache[file_id]
                self.access_order.remove(file_id)
                
    def clear(self):
        """Clear all cache"""
        with self.lock:
            self.cache.clear()
            self.access_order.clear()

class VirtualFileSystem:
    """
    Virtual File System for Decentralized AI Node OS
    Supports hierarchical directories, block storage, caching, and AI/blockchain features
    """
    
    def __init__(self, total_blocks: int = 1024, block_size: int = 4096):
        # Block storage
        self.total_blocks = total_blocks
        self.block_size = block_size
        self.blocks: Dict[int, Block] = {}
        self.free_blocks: set = set(range(total_blocks))
        
        # File system structures
        self.files: Dict[str, FileMetadata] = {}  # file_id -> metadata
        self.directories: Dict[str, Dict[str, DirectoryEntry]] = {}  # dir_path -> entries
        self.path_to_file_id: Dict[str, str] = {}  # path -> file_id
        
        # Caching and performance
        self.cache = FileSystemCache(max_size=100)
        self.access_stats: Dict[str, int] = {}
        
        # Security and encryption
        self.encryption_keys: Dict[str, str] = {}
        self.file_index: Dict[str, List[str]] = {}  # Search index
        
        # Synchronization
        self.fs_lock = threading.RWLock() if hasattr(threading, 'RWLock') else threading.RLock()
        
        # Performance metrics
        self.read_operations = 0
        self.write_operations = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_io_time = 0.0
        
        # Initialize root directory
        self._initialize_root()
        
    def _initialize_root(self):
        """Initialize root directory and system directories"""
        # Create root directory
        self.directories["/"] = {}
        
        # Create system directories
        system_dirs = [
            "/system",
            "/ai_models", 
            "/ai_datasets",
            "/blockchain",
            "/smart_contracts",
            "/network",
            "/users",
            "/tmp"
        ]
        
        for dir_path in system_dirs:
            self.create_directory(dir_path, AccessLevel.ADMIN)
            
    def _allocate_blocks(self, num_blocks: int) -> List[int]:
        """Allocate consecutive blocks for file storage"""
        if len(self.free_blocks) < num_blocks:
            raise OSError("Not enough free blocks")
            
        allocated = []
        for _ in range(num_blocks):
            block_id = self.free_blocks.pop()
            self.blocks[block_id] = Block(block_id=block_id)
            allocated.append(block_id)
            
        return allocated
        
    def _deallocate_blocks(self, block_ids: List[int]):
        """Deallocate blocks and return to free pool"""
        for block_id in block_ids:
            if block_id in self.blocks:
                del self.blocks[block_id]
                self.free_blocks.add(block_id)
                
    def _calculate_blocks_needed(self, size: int) -> int:
        """Calculate number of blocks needed for given size"""
        return (size + self.block_size - 1) // self.block_size
        
    def _normalize_path(self, path: str) -> str:
        """Normalize file path"""
        if not path.startswith("/"):
            path = "/" + path
        return os.path.normpath(path).replace("\\", "/")
        
    def _get_parent_directory(self, path: str) -> str:
        """Get parent directory path"""
        normalized = self._normalize_path(path)
        if normalized == "/":
            return "/"
        return os.path.dirname(normalized)
        
    def _get_filename(self, path: str) -> str:
        """Get filename from path"""
        return os.path.basename(self._normalize_path(path))
        
    def create_file(self, 
                   path: str, 
                   content: bytes = b"", 
                   file_type: FileType = FileType.REGULAR,
                   owner: str = "system",
                   access_level: AccessLevel = AccessLevel.USER,
                   permissions: int = 0o644) -> str:
        """Create a new file"""
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            # Check if file already exists
            if normalized_path in self.path_to_file_id:
                raise FileExistsError(f"File already exists: {normalized_path}")
                
            # Check if parent directory exists
            parent_dir = self._get_parent_directory(normalized_path)
            if parent_dir not in self.directories:
                raise FileNotFoundError(f"Parent directory does not exist: {parent_dir}")
                
            # Generate unique file ID
            file_id = str(uuid.uuid4())
            
            # Calculate blocks needed
            blocks_needed = self._calculate_blocks_needed(len(content))
            
            # Allocate blocks
            try:
                block_list = self._allocate_blocks(blocks_needed) if blocks_needed > 0 else []
            except OSError:
                raise OSError("Insufficient disk space")
                
            # Write content to blocks
            if content:
                self._write_content_to_blocks(block_list, content)
                
            # Create file metadata
            metadata = FileMetadata(
                name=self._get_filename(normalized_path),
                file_type=file_type,
                size=len(content),
                owner=owner,
                access_level=access_level,
                permissions=permissions,
                block_list=block_list
            )
            
            # Set AI/blockchain specific metadata
            if file_type == FileType.AI_MODEL:
                metadata.model_version = f"v{random.randint(1, 10)}.{random.randint(0, 9)}"
                metadata.ai_accuracy = round(random.uniform(0.80, 0.99), 3)
                metadata.pin_in_memory = True
            elif file_type == FileType.BLOCKCHAIN_DATA:
                metadata.blockchain_hash = hashlib.sha256(content).hexdigest()
                metadata.pin_in_memory = True
            elif file_type == FileType.AI_DATASET:
                metadata.dataset_format = "numpy" if b"numpy" in content else "csv"
            elif file_type == FileType.SMART_CONTRACT:
                metadata.smart_contract_version = f"v{random.randint(1, 5)}.{random.randint(0, 9)}.{random.randint(0, 9)}"
            
            # Store file metadata
            self.files[file_id] = metadata
            self.path_to_file_id[normalized_path] = file_id
            
            # Add to parent directory
            filename = self._get_filename(normalized_path)
            self.directories[parent_dir][filename] = DirectoryEntry(
                name=filename,
                is_directory=False,
                file_id=file_id,
                size=len(content)
            )
            
            # Update search index
            self._update_search_index(file_id, normalized_path, content)
            
            self.write_operations += 1
            return file_id
            
    def _write_content_to_blocks(self, block_list: List[int], content: bytes):
        """Write content to allocated blocks"""
        offset = 0
        for block_id in block_list:
            block = self.blocks[block_id]
            chunk_size = min(self.block_size, len(content) - offset)
            block.data = content[offset:offset + chunk_size]
            block.used = True
            block.dirty = True
            block.checksum = block.calculate_checksum()
            offset += chunk_size
            
    def read_file(self, path: str, user_level: AccessLevel = AccessLevel.USER) -> bytes:
        """Read file content"""
        
        start_time = time.time()
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            # Check if file exists
            if normalized_path not in self.path_to_file_id:
                raise FileNotFoundError(f"File not found: {normalized_path}")
                
            file_id = self.path_to_file_id[normalized_path]
            metadata = self.files[file_id]
            
            # Check permissions
            if not metadata.has_permission(user_level, FilePermission.READ):
                raise PermissionError(f"Access denied: {normalized_path}")
                
            # Check cache first
            cached_data = self.cache.get(file_id)
            if cached_data is not None:
                self.cache_hits += 1
                metadata.accessed_time = time.time()
                self.access_stats[file_id] = self.access_stats.get(file_id, 0) + 1
                return cached_data
                
            self.cache_misses += 1
            
            # Read from blocks
            content = b""
            for block_id in metadata.block_list:
                if block_id in self.blocks:
                    block = self.blocks[block_id]
                    
                    # Verify block integrity
                    if not block.verify_integrity():
                        raise IOError(f"Block integrity check failed: {block_id}")
                        
                    content += block.data
                    block.last_accessed = time.time()
                    
            # Trim to actual file size
            content = content[:metadata.size]
            
            # Update cache - always cache files that are read
            self.cache.put(file_id, content)
                
            # Update access statistics
            metadata.accessed_time = time.time()
            self.access_stats[file_id] = self.access_stats.get(file_id, 0) + 1
            
            self.read_operations += 1
            self.total_io_time += time.time() - start_time
            
            return content
            
    def write_file(self, 
                  path: str, 
                  content: bytes, 
                  user_level: AccessLevel = AccessLevel.USER,
                  append: bool = False) -> bool:
        """Write content to file"""
        
        start_time = time.time()
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            # Check if file exists
            if normalized_path not in self.path_to_file_id:
                raise FileNotFoundError(f"File not found: {normalized_path}")
                
            file_id = self.path_to_file_id[normalized_path]
            metadata = self.files[file_id]
            
            # Check permissions
            if not metadata.has_permission(user_level, FilePermission.WRITE):
                raise PermissionError(f"Write access denied: {normalized_path}")
                
            if append:
                # Append mode: read existing content and append new content
                existing_content = b""
                for block_id in metadata.block_list:
                    if block_id in self.blocks:
                        existing_content += self.blocks[block_id].data
                existing_content = existing_content[:metadata.size]
                content = existing_content + content
                
            # Deallocate existing blocks
            self._deallocate_blocks(metadata.block_list)
            
            # Calculate new blocks needed
            blocks_needed = self._calculate_blocks_needed(len(content))
            
            # Allocate new blocks
            try:
                new_blocks = self._allocate_blocks(blocks_needed) if blocks_needed > 0 else []
            except OSError:
                raise OSError("Insufficient disk space")
                
            # Write content to new blocks
            if content:
                self._write_content_to_blocks(new_blocks, content)
                
            # Update metadata
            metadata.block_list = new_blocks
            metadata.size = len(content)
            metadata.modified_time = time.time()
            
            # Update directory entry
            parent_dir = self._get_parent_directory(normalized_path)
            filename = self._get_filename(normalized_path)
            if filename in self.directories[parent_dir]:
                self.directories[parent_dir][filename].size = len(content)
                self.directories[parent_dir][filename].modified_time = time.time()
                
            # Update cache
            self.cache.put(file_id, content)
            
            # Update search index
            self._update_search_index(file_id, normalized_path, content)
            
            self.write_operations += 1
            self.total_io_time += time.time() - start_time
            
            return True
            
    def delete_file(self, path: str, user_level: AccessLevel = AccessLevel.USER) -> bool:
        """Delete a file"""
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            # Check if file exists
            if normalized_path not in self.path_to_file_id:
                raise FileNotFoundError(f"File not found: {normalized_path}")
                
            file_id = self.path_to_file_id[normalized_path]
            metadata = self.files[file_id]
            
            # Check permissions
            if not metadata.has_permission(user_level, FilePermission.WRITE):
                raise PermissionError(f"Delete access denied: {normalized_path}")
                
            # Deallocate blocks
            self._deallocate_blocks(metadata.block_list)
            
            # Remove from file system structures
            del self.files[file_id]
            del self.path_to_file_id[normalized_path]
            
            # Remove from parent directory
            parent_dir = self._get_parent_directory(normalized_path)
            filename = self._get_filename(normalized_path)
            if filename in self.directories[parent_dir]:
                del self.directories[parent_dir][filename]
                
            # Remove from cache
            self.cache.remove(file_id)
            
            # Remove from search index
            self._remove_from_search_index(file_id)
            
            return True
            
    def create_directory(self, path: str, access_level: AccessLevel = AccessLevel.USER) -> bool:
        """Create a new directory"""
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            # Check if directory already exists
            if normalized_path in self.directories:
                raise FileExistsError(f"Directory already exists: {normalized_path}")
                
            # Check if parent directory exists (except for root)
            if normalized_path != "/":
                parent_dir = self._get_parent_directory(normalized_path)
                if parent_dir not in self.directories:
                    raise FileNotFoundError(f"Parent directory does not exist: {parent_dir}")
                    
                # Add to parent directory
                dirname = self._get_filename(normalized_path)
                self.directories[parent_dir][dirname] = DirectoryEntry(
                    name=dirname,
                    is_directory=True,
                    file_id="",  # Directories don't have file IDs
                    size=0
                )
                
            # Create directory
            self.directories[normalized_path] = {}
            
            return True
            
    def list_directory(self, path: str = "/") -> List[DirectoryEntry]:
        """List directory contents"""
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            if normalized_path not in self.directories:
                raise FileNotFoundError(f"Directory not found: {normalized_path}")
                
            return list(self.directories[normalized_path].values())
            
    def get_file_info(self, path: str) -> FileMetadata:
        """Get file metadata"""
        
        with self.fs_lock:
            normalized_path = self._normalize_path(path)
            
            if normalized_path not in self.path_to_file_id:
                raise FileNotFoundError(f"File not found: {normalized_path}")
                
            file_id = self.path_to_file_id[normalized_path]
            return self.files[file_id]
            
    def _update_search_index(self, file_id: str, path: str, content: bytes):
        """Update search index for file content"""
        # Simple keyword extraction for search
        try:
            text_content = content.decode('utf-8', errors='ignore').lower()
            words = set(text_content.split())
            
            # Index filename
            filename = self._get_filename(path).lower()
            words.add(filename)
            
            # Update index
            for word in words:
                if len(word) > 2:  # Skip very short words
                    if word not in self.file_index:
                        self.file_index[word] = []
                    if file_id not in self.file_index[word]:
                        self.file_index[word].append(file_id)
                        
        except Exception:
            pass  # Skip indexing if content is not text
            
    def _remove_from_search_index(self, file_id: str):
        """Remove file from search index"""
        for word, file_list in self.file_index.items():
            if file_id in file_list:
                file_list.remove(file_id)
                
    def search_files(self, query: str) -> List[Tuple[str, FileMetadata]]:
        """Search files by content and filename"""
        results = []
        query_words = query.lower().split()
        
        # Find files containing any of the query words
        matching_files = set()
        for word in query_words:
            if word in self.file_index:
                matching_files.update(self.file_index[word])
                
        # Convert file IDs to paths and metadata
        for file_id in matching_files:
            for path, fid in self.path_to_file_id.items():
                if fid == file_id:
                    results.append((path, self.files[file_id]))
                    break
                    
        return results
        
    def get_file_system_stats(self) -> Dict[str, Any]:
        """Get comprehensive file system statistics"""
        total_files = len(self.files)
        total_directories = len(self.directories)
        used_blocks = self.total_blocks - len(self.free_blocks)
        
        # Calculate storage by file type
        storage_by_type = {}
        for metadata in self.files.values():
            file_type = metadata.file_type.value
            storage_by_type[file_type] = storage_by_type.get(file_type, 0) + metadata.size
            
        # Cache statistics
        cache_hit_rate = self.cache_hits / max(self.cache_hits + self.cache_misses, 1) * 100
        
        return {
            "total_files": total_files,
            "total_directories": total_directories,
            "used_blocks": used_blocks,
            "free_blocks": len(self.free_blocks),
            "storage_utilization": (used_blocks / self.total_blocks) * 100,
            "total_storage": self.total_blocks * self.block_size,
            "used_storage": used_blocks * self.block_size,
            "storage_by_type": storage_by_type,
            "read_operations": self.read_operations,
            "write_operations": self.write_operations,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": cache_hit_rate,
            "total_io_time": self.total_io_time,
            "avg_io_time": self.total_io_time / max(self.read_operations + self.write_operations, 1),
            "indexed_words": len(self.file_index)
        } 